# ============================================
# Yojana-AI Environment Configuration
# ============================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env file with real credentials to git
# ============================================

# ============================================
# LLM API KEYS
# ============================================

# Groq API Key (for cloud-based LLM inference)
# Get your key from: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# ============================================
# VECTOR DATABASE
# ============================================

# Qdrant Cloud Configuration
# Get your credentials from: https://cloud.qdrant.io/
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key_here

# Qdrant Collection Name
QDRANT_COLLECTION=myscheme_rag

# ============================================
# OLLAMA CONFIGURATION (Local LLM)
# ============================================

# Ollama Base URL
# For local development: http://localhost:11434
# For Docker Compose: http://ollama:11434
OLLAMA_BASE_URL=http://localhost:11434

# ============================================
# MODEL CONFIGURATION
# ============================================

# Embedding Model
EMBEDDING_MODEL=BAAI/bge-m3

# Ollama Models (for local inference)
OLLAMA_MODEL=phi3.5:3.8b
CHUNKING_MODEL=llama3.1:8b

# Groq Model (for cloud inference)
GROQ_MODEL=llama-3.3-70b-versatile

# LLM Temperature
TEMPERATURE=0.2

# ============================================
# RETRIEVAL CONFIGURATION
# ============================================

# Top K documents to retrieve
TOP_K=5

# Minimum similarity score threshold
MIN_SIMILARITY_SCORE=0.5

# Hybrid Retrieval Weights
HYBRID_RETRIEVAL_ENABLED=true
BM25_WEIGHT=0.4
SEMATIC_WEIGHT=0.6
RRF_K=60

# ============================================
# RAG WORKFLOW LIMITS
# ============================================

# Maximum reflection iterations (Self-RAG)
MAX_REFLECTION_ITERATIONS=2

# Maximum correction iterations (Corrective RAG)
MAX_CORRECTION_ITERATIONS=2

# ============================================
# API CONFIGURATION
# ============================================

# FastAPI Server
API_HOST=0.0.0.0
API_PORT=8000

# CORS Origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# ============================================
# LOGGING CONFIGURATION
# ============================================

# Log Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log Directory
LOG_DIR=logs

# ============================================
# AWS CONFIGURATION (for deployment)
# ============================================

# AWS Region
AWS_REGION=ap-south-1

# AWS Credentials (for CI/CD)
# Note: Use IAM roles for EC2 instances instead of hardcoding
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key

# ECR Repository
ECR_REGISTRY=your-account-id.dkr.ecr.ap-south-1.amazonaws.com
ECR_REPOSITORY=yojana-ai

# ============================================
# JENKINS CONFIGURATION (for CI/CD)
# ============================================

# Jenkins Server URL
JENKINS_URL=http://your-jenkins-server:8080

# Jenkins Credentials
JENKINS_USER=your_jenkins_user
JENKINS_TOKEN=your_jenkins_api_token

# ============================================
# DEPLOYMENT SERVER (EC2)
# ============================================

# Application Server IP
APP_SERVER_IP=your-ec2-public-ip

# SSH Configuration
APP_SERVER_USER=ec2-user
APP_SERVER_SSH_KEY=/path/to/your/key.pem

# Deployment Directory
DEPLOYMENT_DIR=/opt/yojana-ai

# ============================================
# NOTIFICATION CONFIGURATION (Optional)
# ============================================

# Slack Webhook URL (for deployment notifications)
# SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# ============================================
# FEATURE FLAGS
# ============================================

# Enable structured output with schema validation
STRUCTURED_OUTPUT_ENABLED=true

# Enable adaptive threshold
ADAPTIVE_THRESHOLD_ENABLED=true

# Enable metadata filtering
METADATA_FILTERING_ENABLED=true

# ============================================
# DEVELOPMENT vs PRODUCTION
# ============================================

# Environment: development, staging, production
ENVIRONMENT=development

# Debug mode
DEBUG=false

# ============================================
# NOTES
# ============================================
# 1. For local development:
#    - Set OLLAMA_BASE_URL=http://localhost:11434
#    - Ensure Ollama is running locally
#
# 2. For Docker Compose deployment:
#    - Set OLLAMA_BASE_URL=http://ollama:11434
#    - Models will be shared via Docker volume
#
# 3. For production on AWS:
#    - Use AWS Secrets Manager for sensitive data
#    - Use IAM roles instead of hardcoded credentials
#    - Set appropriate CORS_ORIGINS
#
# 4. GitHub Actions Secrets Required:
#    - GROQ_API_KEY
#    - QDRANT_URL
#    - QDRANT_API_KEY
#    - AWS_ACCESS_KEY_ID
#    - AWS_SECRET_ACCESS_KEY
#    - JENKINS_URL
#    - JENKINS_USER
#    - JENKINS_TOKEN
#    - APP_SERVER_IP
#
# 5. Jenkins Credentials Required:
#    - ecr-registry-url
#    - app-server-ip
#    - app-server-ssh-key
# ============================================
