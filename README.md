# Government Schemes RAG System

[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![Terraform](https://img.shields.io/badge/Terraform-AWS-purple.svg)](https://www.terraform.io/)
[![LangGraph](https://img.shields.io/badge/LangGraph-Multi--Agent-orange.svg)](https://langchain-ai.github.io/langgraph/)

Production-ready multi-agent RAG system for querying **2,153 Indian government schemes** from [myscheme.gov.in](https://www.myscheme.gov.in) with adaptive retrieval and self-correction.

---

## ğŸ¯ Features

### ğŸ§  Intelligent Routing
Automatic query classification into 6 intent categories:
- `DISCOVERY` - Finding relevant schemes
- `ELIGIBILITY` - Checking who can apply  
- `BENEFITS` - Understanding subsidy amounts
- `COMPARISON` - Comparing multiple schemes
- `PROCEDURE` - Learning application process
- `GENERAL` - Fallback queries

### ğŸ”„ Self-Correcting RAG
- **Self-RAG**: Judges retrieval relevance, refines queries when docs aren't sufficient
- **Corrective RAG**: Validates answer quality, re-retrieves if answers are vague/incomplete
- **Adaptive Loops**: Multiple refinement cycles until quality threshold met

### âš¡ Performance
- **BGE-M3 Embeddings**: 1024-dim multilingual embeddings
- **Qdrant Vector DB**: Fast similarity search over 10,812 chunks
- **Hybrid LLM Strategy**: Ollama (local) + Groq (cloud) for optimal cost/performance
- **LangGraph Orchestration**: Efficient state machine for multi-agent workflow

### ğŸ›ï¸ Production Ready
- **FastAPI** with Swagger/ReDoc docs
- **Docker** containerization
- **Terraform** for AWS ECS deployment
- **CloudWatch** logging and monitoring
- **Custom exceptions** and error handling
- **Health checks** and graceful degradation

---

## ğŸ—ï¸ System Architecture

### Hybrid LLM Strategy

| Task | Model | Provider | Reason |
|------|-------|----------|--------|
| **Data Chunking** | llama3.1:8b | Ollama (local) | One-time job, cost-effective |
| **Intent Classification** | deepseek-r1:8b | Ollama (local) | Lightweight, fast |
| **Query Refinement** | deepseek-r1:8b | Ollama (local) | Adaptive, frequent |
| **Answer Generation** | llama-3.3-70b | Groq (cloud) | High quality, fast inference |
| **Relevance Judging** | llama-3.3-70b | Groq (cloud) | Critical path, accuracy |

### RAG Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          User Query                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Intent Classify  â”‚  [Ollama: deepseek-r1:8b]
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Vector Retrieve  â”‚  [BGE-M3 + Qdrant]
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Relevance Judge  â”‚  [Groq: llama-3.3-70b]
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
   Not Relevant   Relevant
        â”‚       â”‚       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
â”‚  Refine   â”‚ â”‚  Generate â”‚
â”‚  Query    â”‚ â”‚  Answer   â”‚  [Groq: llama-3.3-70b]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  [Ollama]          â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Quality Check      â”‚  [Groq: llama-3.3-70b]
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
         Inadequate   Good
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”   â”‚
      â”‚ Correctiveâ”‚   â”‚
      â”‚ Re-retrieve   â”‚  [Ollama: deepseek-r1:8b]
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Final Answer  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
git clone https://github.com/pranaya-mathur/govt-schemes-rag.git
cd govt-schemes-rag
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cat > .env << EOF
# Groq (for answer generation)
GROQ_API_KEY=your_groq_key

# Qdrant (vector database)
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your_qdrant_key

# Ollama (for adaptive tasks)
OLLAMA_BASE_URL=http://localhost:11434

# Logging
LOG_LEVEL=INFO
EOF
```

### 3. Start Ollama

```bash
# Pull models
ollama pull deepseek-r1:8b
ollama pull llama3.1:8b

# Start server
ollama serve
```

### 4. Process Data (First Time Only)

See **[data_pipeline/README.md](data_pipeline/README.md)** for complete data processing guide.

```bash
# Run complete pipeline: Load -> Chunk -> Index
python data_pipeline/run_pipeline.py path/to/schemes.json
```

### 5. Start API

```bash
python -m uvicorn api.app:app --reload
```

API available at: http://localhost:8000

### 6. Docker Deployment

```bash
docker-compose up -d
```

---

## ğŸ“ API Usage

### Query Endpoint

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "subsidy schemes for small entrepreneurs",
    "top_k": 5
  }'
```

### Example Queries

```python
import requests

queries = [
    "What are the manufacturing subsidy schemes?",      # DISCOVERY
    "Can women entrepreneurs apply for PMEGP?",         # ELIGIBILITY
    "How much subsidy does Startup India provide?",    # BENEFITS
    "Compare MSME schemes vs Startup India",           # COMPARISON
    "How do I apply for CGTMSE loan guarantee?"        # PROCEDURE
]

for query in queries:
    response = requests.post(
        "http://localhost:8000/query",
        json={"query": query}
    )
    result = response.json()
    print(f"Intent: {result['intent']}")
    print(f"Answer: {result['answer']}\n")
```

### Response Format

```json
{
  "query": "subsidy schemes for small entrepreneurs",
  "intent": "DISCOVERY",
  "answer": "Here are relevant schemes...",
  "retrieved_docs": [
    {
      "id": "123",
      "score": 0.87,
      "scheme_name": "PMEGP",
      "theme": "benefits",
      "text": "...",
      "official_url": "https://..."
    }
  ],
  "needs_reflection": false,
  "needs_correction": false
}
```

---

## ğŸ“Š Data Pipeline

Complete pipeline for processing government schemes:

```
Raw Schemes (JSON) â†’ LLM Chunking â†’ BGE-M3 Embeddings â†’ Qdrant Index
```

### LLM-Powered Chunking

- **Model**: `llama3.1:8b` via ChatOllama
- **Strategy**: Theme-based intelligent splitting
- **Themes**: benefits, eligibility, application-steps, documents, contact, general
- **Output**: 10,812 chunks from 2,153 schemes

### Indexing

- **Embeddings**: BGE-M3 (1024-dim)
- **Vector DB**: Qdrant
- **Distance**: Cosine similarity

See **[data_pipeline/README.md](data_pipeline/README.md)** for detailed guide.

---

## ğŸ§ª Tech Stack

| Component | Technology |
|-----------|------------|
| **Orchestration** | LangGraph |
| **LLM Framework** | LangChain |
| **Inference** | Groq (llama-3.3-70b) + Ollama (deepseek-r1:8b, llama3.1:8b) |
| **Vector DB** | Qdrant |
| **Embeddings** | BGE-M3 (sentence-transformers) |
| **API** | FastAPI |
| **Containerization** | Docker |
| **Infrastructure** | Terraform (AWS ECS) |
| **Monitoring** | CloudWatch |

---

## ğŸ’¾ Project Structure

```
govt-schemes-rag/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py              # FastAPI application
â”‚   â””â”€â”€ models.py           # Pydantic schemas
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings.py       # BGE-M3 wrapper
â”‚   â”œâ”€â”€ retrieval.py        # Qdrant client
â”‚   â”œâ”€â”€ llm.py             # Hybrid LLM setup
â”‚   â”œâ”€â”€ prompts.py         # Prompt templates
â”‚   â”œâ”€â”€ nodes.py           # LangGraph nodes
â”‚   â”œâ”€â”€ graph.py           # Workflow definition
â”‚   â”œâ”€â”€ exceptions.py      # Custom exceptions
â”‚   â””â”€â”€ logger.py          # Logging config
â”œâ”€â”€ data_pipeline/          # â­ Data processing
â”‚   â”œâ”€â”€ chunking.py         # LLM-powered chunking
â”‚   â”œâ”€â”€ indexing.py         # Qdrant indexing
â”‚   â”œâ”€â”€ run_pipeline.py     # Complete pipeline
â”‚   â”œâ”€â”€ config.py           # Pipeline config
â”‚   â””â”€â”€ README.md           # Pipeline docs
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf            # AWS infrastructure
â”‚   â”œâ”€â”€ variables.tf       # Terraform variables
â”‚   â””â”€â”€ outputs.tf         # Infrastructure outputs
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ test_queries.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py
â”œâ”€â”€ main.py
â”œâ”€â”€ DEPLOYMENT.md          # Deployment guide
â””â”€â”€ README.md
```

---

## ğŸ”§ Development

### API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### View Logs

```bash
tail -f logs/rag_system.log
```

### Run Tests

```bash
python examples/test_queries.py
```

---

## ğŸ“¦ Deployment

For complete deployment instructions:
- Local development setup
- Docker deployment
- AWS ECS with Terraform
- Monitoring and scaling

See **[DEPLOYMENT.md](DEPLOYMENT.md)**

---

## ğŸ“ Documentation

- **[README.md](README.md)** - Main documentation (this file)
- **[data_pipeline/README.md](data_pipeline/README.md)** - Data processing guide
- **[DEPLOYMENT.md](DEPLOYMENT.md)** - Deployment guide

---

## ğŸ“ License

MIT License

---

## ğŸ’¬ Contact

Built with â¤ï¸ for Indian entrepreneurs by [Pranay Mathur](https://github.com/pranaya-mathur)

---

**â­ Star this repo if you find it useful!**
